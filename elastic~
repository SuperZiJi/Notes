https://www.elastic.co/guide/cn/elasticsearch/guide/current/intro.html


１．Elaticsearch是一个实时的分布式搜索分析引擎。

２．Elaticsearch是一个开源的搜索引擎，建立在Apache Lucene（全文搜索引擎库）之上。

３．为什么不直接使用Apache Lucene？
	因为Lucene非常复杂，要和java做集成需要很大的知识背景。

４．Elaticsearch使用java编写，内部使用Lucene做索引和搜索，隐藏Lucene复杂性，提供简单restful api

５．java api：在代码中你可以使用 Elasticsearch 内置的两个客户端：
	节点客户端（node client）:作为非数据节点加入本地集群，不保存任何数据，但是它知道数据在哪，并且可以把请求转发到正确的节点

	传输客户端（Transport client）:不加入集群，但是可以吧请求转发到集群的一个节点上

６．存储数据到Elasticsearch的行为叫　索引，索引可以包含多个类型
	索引（名词）：类似传统关系数据库的一个数据库，是一个存储关系型文档的地方
	索引（动词）：索引一个文档，就是存储一个文档到一个索引（名词）中。
	倒排索引：用于提升数据检索速度，类似关系型数据库加索引
	默认：一个文档的每一个属性都是被索引的（有一个倒排索引）

７．轻量搜索指令：
	查询所有：GET /megacorp/employee/_search
	带参数查询：GET /megacorp/employee/_search?q=last_name:Smith

８．使用查询表达式搜索
	GET /megacorp/employee/_search
	{
	    "query" : {
		"match" : {
		    "last_name" : "Smith"
		}
	    }
	}

９．更复杂的搜索，使用过滤器
	GET /megacorp/employee/_search
	{
	    "query" : {
		"bool": {
		    "must": {
		        "match" : {
		            "last_name" : "smith" 
		        }
		    },
		    "filter": {
		        "range" : {
		            "age" : { "gt" : 30 } 
		        }
		    }
		}
	    }
	}

１０．ElasticSearch会按照相关性得分排序，从高到低，第一个是最匹配的。
	引入相关性概念：并不是全部匹配，而是按照词汇匹配，可以仅仅匹配部分，但是必须是完整单词

１１．短语搜索：使用match_phrase
	GET /megacorp/employee/_search
	{
	    "query" : {
		"match_phrase" : {
		    "about" : "rock climbing"
		}
	    }
	}
１２．高亮搜索
	GET /megacorp/employee/_search
	{
	    "query" : {
		"match_phrase" : {
		    "about" : "rock climbing"
		}
	    },
	    "highlight": {
		"fields" : {
		    "about" : {}
		}
	    }
	}

１３．分析，ElasticSearch有一个功能叫聚合，aggregations，允许我们基于数据生成一些惊喜的分析结果。聚合与sql中的group by类似，但是更加强大。

１４．文档的元数据
	_index 索引（文档在哪存放）
	_type　类型（文档是什么类型），允许索引下进行逻辑分区，因此需要type
	_id　id唯一，是一个字符串，使用这三个可以唯一的确定一个文档

１５．创建索引文档：使用自定义id或自动id
	自定义：put /{index}/{type}/{id}
	自动：put /{index}/{type}   自动id是url-safe，由可修改的flakeId模式生成,基于base64编码且长度为２０字符串

１６．取回一个文档
	get /website/blog/{id}?pretty
	
	包含文档元数据：_index,_type,_id,_version,还有_source:创建文档时的原始json
	pretty让打印出的响应更加可读

１７．取回文档的一部分		
	get /website/blog/{id}?_source={field1},{field2}
	如果你只想要_source字段，不需要任何元数据，你可以使用_source端点
	get /website/blog/{id}/_source

１８．检查文档是否存在
	只检查是否存在，使用HEAD代替GET,返回200代表存在，404不存在

１９．更新整个文档
	更新现有文档只需要put　不过需要指定id，在响应中，_version会增加１，_created会变成false
	在内部，ElaticSearch将旧的删除，但是不是立即删除，会在后台清理这些删除数据。

２０．创建新文档
	如何保证我是创建新的文档而不是覆盖旧的文档？
	1.使用自动id,保证了id不重复
	2.指定操作类型
		PUT /website/blog/123?op_type=create
		PUT /website/blog/123/_create


２１.删除文档
	DELETE /website/blog/123
	成功删除响应中会把_version＋１
	未找到，响应404，但是_version依然＋１
	删除文档只是把状态改为已删除，随着数据量的增大，会在后台清理已删除的文档

２２．乐观并发控制
	内部version:
		PUT /website/blog/1?version=1 
		通过指定version的方式来控制，当version不相等时，报冲突错
	外部verison:
		PUT /website/blog/2?version=5&version_type=external
		每次的version必须大于上次的version，否则报冲突错

２３．更新部分文档
	POST /{index}/{type}/{id}/_update
	{
	   "doc" : {
	      "field":value
	   }
	}
	
	脚本模式：
	POST /website/blog/1/_update
	{
	   "script" : "ctx._source.views+=1"
	}

24.ElasticSearch可以通过多次请求合并成一次，来增加效率
	使用 GET /_mget
	docs数组作为参数，每个参数里包含文档的元数据(必须包含index 和 id否则报错)，可以用Source指定返回多个字段
	{
	"docs":[
		{
			"_index":"",
			"_type":"",
			"_id":"",
			"_source":""
		}
		]	
	}
	
	如果想检索的数据都在相同的_index（甚至相同的_type），则可以在url中指定默认的index和type
	使用GET /{index}/{type}/_mget，当然，你也可以使用docs参数来覆盖掉url中的index和type
	当index和type确定时，你可以只传一个ids数组{"ids":["2","1"]}
	注意，当文档找不到时也会返回200,只是返回的数据里 found为false，且source为空

25.代价较小的批量操作
	bulk可以把多个操作放到一起.
	POST /_bulk
	{"action":{"_index":"","_type":"","_id":""}}
	{reqBody}

	{"action":{"_index":"","_type":"","_id":""}}
	{reqBody}

	{"action":{"_index":"","_type":"","_id":""}}
	{reqBody}
	注意：必须换行
	返回结果按照请求顺序排序，每个子请求都是独立执行，不会对其它的造成影响，但是如果有子请求错误，
	返回值顶层的error=true
	
	这意味着bulk的整体不是原子的



26.关于聚合出错，需要开启fielddata
	CURL PUT http://localhost:9200/youindex/_mapping/youtype
	{
		"properties":{
			"goodsName": {
			"type": "text",
			"fielddata": true
			}
		}
	}

27.精确值和全文
	ec中数据概括的分为两类：精确值和全文
	精确值：精确匹配
	全文：模糊匹配，不仅如此，还希望可以理解我们的查询意图。ec会首先分析文档，之后根据结果创建倒排索引。

	倒排索引：倒排索引是一种结构，用于快速的全文搜索，由文档中所有不重复词的列表构成。
		  doc_1:The quick brown fox jumped over the lazy dog 
		  doc_2:Quick brown foxes leap over lazy dogs in summer 
		将每个文档的content拆分成单独的词，创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪
		个文档
		问题:对于quick和Quick用户可能认为这两个是相同的，所以ec会把所有的词条规范为标准模式
		例如：Quick变成quick foxes变成fox
	分析与分析器：
		1.把文本分成适合倒排索引的独立词条
		2.把词条统一化为标准格式，提高可搜索性
		分析器用下面三个功能执行上面的工作：
			字符过滤器：字符串按照顺序，通过每个字符过滤器。在分词前整理字符串，一个字符过滤器可以用来
				去掉html,或者把&转化成and
			分词器：把字符串分成单个词条，简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。
			Token过滤器：词条按照顺序通过每个token过滤器，这个过程可能会改变词条，例如：小写化Quick,
				删除词条（例如，a,and，the等无用词），或者增加词条（例如,jump和liap这种同义词）
		
		内置分析器：测试字符串：Set the shape to semi-transparent by calling set_trans(5)
		
		1.标准分析器：是ec默认分析器，根据unicode联盟定义的单词边界，划分文本，删除绝大部分标点，最后把词
				条小写。
			结果：set, the, shape, to, semi, transparent, by, calling, set_trans, 5
		2.简单分析器：在任何不是字母的地方分隔文本，然后将词条小写
			结果：set, the, shape, to, semi, transparent, by, calling, set, trans
		3.空格分析器：在空格的地方划分文本
			结果：Set, the, shape, to, semi-transparent, by, calling, set_trans(5)
		4.语言分析器：有多种语言，可以考虑语言的特点。例如英语分析器，会把and，the删除，还可以提取英语单词
			的词干。
			结果：set, shape, semi, transpar, call, set_tran, 5
		什么时候使用分析器：当我们索引一个文档，他的全文域被分析成词条，创建倒排索引。当我们在全文域搜索的
			时候，需要把查询字符串通过相同的分析过程。保证格式的一致性。
		当你查询一个全文域的时候，会对查询字符串应用相同的分析器
		当你查询一个精确域的时候，不会分析查询字符串，而是搜索你制定的精确值。

		测试分析器：
			GET /_analyze
			{
			  "analyzer": "standard",
			  "text": "Text to analyze"
			}

28.映射
	1.为了能够将时间域视为时间，数字域视为数字，为了知道每个域中数据的类型。这个信息就包含在映射中。
	2.索引中每个文档都有自己的类型。每一种类型都有自己的映射或者模式定义。映射定义了类型中的域，每个域的数据类
		型，以及ec该如何处理这些域。
	3.核心简单域类型：
	    字符串: string
	    整数 : byte, short, integer, long
	    浮点数: float, double
	    布尔型: boolean
	    日期: date 
	当你索引一个包含新域的文档，ec会使用动态映射，猜测域类型，使用如下规则
		布尔：boolean
		整数：long
		浮点数：double
		字符串：有效日期2018-05-24:date
		字符串：football：string
	这意味着，如果你通过引号（"132"）索引一个数字，他会被映射为string类型而不是long。
	但是如果这个域已经映射为long，那么ec会尝试将这个字符串转化为long,如果无法转化就会抛出异常。
	4.查看映射：GET /{index}/_mapping/{type}
	5.自定义域映射：
		对于不是string的域，只需要设置type
		对于string类型的域，需要设置index和analyzer
	6.复杂核心域类型
		多值域：一个域包含多个值，可以用数组的形式索引标签
		空域：空域不会被索引 null,[],[null]
		多层级对象：
		内部对象的映射：
			内部对象会被扁平化处理，例如：
			{"user":[{"name":"ziji","age":22},{"name":"hah","age":20}]}
			{"user.name":[ziji,hah],"user.age":[22,20]}
			这样就导致name和age的关联关系丢失
		嵌套对象：为了解决上述问题，我们需要把user字段类型设置为nested，这样每一个嵌套对象都会被索引为一个
		隐藏的独立文档。

29.请求体查询
	1.空查询
		GET /_search
	2.只用一个查询字符串，就可以指定多个index和多个type查询
		GET /index_2014*/type1,type2/_search
	  同时可以用from和size来做分页
		{
		  "from": 30,
		  "size": 10
		}
	  get请求是可以带body体的，看服务器支不支持(RFC 7231有描述)

30.查询表达式（Query DSL）
	ec中的查询语言。只需要将查询语句传递给query参数
	GET /_search
	{
		"query":my query			
	}
	1.match:	match:{"field":"value"}
	2.合并查询语句：一条符合语句合一合并任何其它的查询语句，包括复合语句，意味着复合语句之间可以相互嵌套，可以
		表达非常复杂的逻辑
	例如：仅仅观察结构就行
		{
		    "bool": {
			"must": { "match":   { "email": "business opportunity" }},
			"should": [
			    { "match":       { "starred": true }},
			    { "bool": {
				"must":      { "match": { "folder": "inbox" }},
				"must_not":  { "match": { "spam": true }}
			    }}
			],
			"minimum_should_match": 1
		    }
		}
31.查询与过滤
	DSL拥有一套查询组件，这些组件可以无限组合，可以在以下两种情况下使用：过滤、查询。
	1.过滤情况：查询仅仅问一个问题，是否匹配？,回答很简单：yes/no
	2.查询情况：查询变成了一个“评分”查询，它不光要判断这个文档是否匹配，还要判断这个我文档的匹配程度如何。
		相关性字段：_score,并且按照这个字段进行排序。
		注意：filters已经从技术上被排除了，同事所有的queries都拥有变成不评分查询的能力
	为了明确：我们使用filter这个词表示不评分，只过滤情况下的查询。（filter,filter query non-scoring query）
		这几个词是相同的（只过滤查询）
		单独使用query代表评分查询（scoring query)

	3.性能差异：
		filters:计算非常快，经常使用，结果会被缓存到内存中，有多种方法可以优化查询结果。
		query:计算较为耗时，查询结果不缓存。
		注意：由于倒排索引：简单的query在匹配少量文档时可以与一个涵盖了百万文档的filter表现的一样好。
	
32.最重要的几个查询
	1.match_all：匹配所有文档，是默认的查询 {"match_all":{}}
	2.match:具体到某个字段 {"match":{"field":value}}
	3.multi_match：在多个字段执行相同的match 
		{
		    "multi_match": {
			"query":    "full text search",
			"fields":   [ "title", "body" ]
		    }
		}
	4.range：区间查询（数字或者时间）
		{
		    "range": {
			"age": {
			    "gte":  20,
			    "lt":   30
			}
		    }
		}
	5.term查询：精确值匹配，数字、时间、布尔或者那些not_analyzed的字符串
		{ "term": { "age":    26           }}
	6.terms查询：和term类似，但是允许多值匹配
		{ "terms": { "tag": [ "search", "full_text", "nosql" ] }}
	7.exists和missing查询：查询制定字段有值或者无值

33.组合多查询
	可以使用bool查询来实现组合查询需求
	bool可以接收以下参数：
	1.must:文档必须匹配这些条件才能被包含进来
	2.must_not：文档必须不匹配这些条件才能被包含进来
	3.should：满足这些语句中的任意语句，增加_score得分，否则无影响，用于修正相关性得分
	4.filter：必须匹配，但是不评分，可以用于屏蔽某个field的评分
		如果需要多个不同的标准，可以把bool放入filter语句中
	5.constant_score:将一个不变的常量评分 1 应用于所有匹配的文档，常常用于只需要执行一个filter的情况下

34.验证查询
	由于验证可以变得很复杂，尤其是不同分析器和不同字段映射相结合时
	这时可以用validate-query API来验证查询是否合法。
	GET /gb/tweet/_validate/query?explain

35.排序：
	可以组合排序，并且先按照上面的排序，就是date
	"sort": [
		{ "date":   { "order": "desc" }},
		{ "_score": { "order": "desc" }}
	    ]
	特殊情况：一个字段有多个值，可以把多值变成单个值：min、max、avg、sum排序模式来实现
	例如：
		"sort": {
		    "dates": {
			"order": "asc",
			"mode":  "min"
		    }
		}

36.相关性
	什么是相关性，相关性如何计算？
	1.每个文档都有相关性评分，用一个正浮点数字段_score来表示，_score的评分越高，相关性就越高。
	2.评分的计算方式取决于查询类型：
		fuzzy：计算关键词的拼写相似程度
		terms：计算找到的内容域关键词组组成部分匹配的百分比
